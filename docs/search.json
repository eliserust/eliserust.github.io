[
  {
    "objectID": "thesis.html",
    "href": "thesis.html",
    "title": "Undergraduate Honors Thesis",
    "section": "",
    "text": "As part of my undergraduate major - Quantitative Social Sciences (QSS) - I wrote a culminating honors thesis examining groundwater well overdraft in California. Expanding on work I did as a Spatial Data Science intern for the Nature Conservancy in 2019, I was interested in the 6 million Californians who rely on wells that tap into groundwater below as their primary source of fresh water. As the water table in the state continues to lower, the risk of “overdraft” - or the wells going dry - increases for these residents.\nThus, the thesis takes an econometrics approach to examining correlations between socioeconomic and demographic characteristics of California residents - and their risk of “overdraft.” Data from the US Government, State governments, academic institutions, and nonprofits was scraped, cleaned, and examined using Python, R, and QGIS. A measure of overdraft was self-generated for this project, using well depth data and water table data after serious spatial interpolation took place.\nThe thesis can be read here:"
  },
  {
    "objectID": "fake_news.html",
    "href": "fake_news.html",
    "title": "Fraudulent News Detection",
    "section": "",
    "text": "Code can be found here:\nPoster:"
  },
  {
    "objectID": "cherry_blossoms.html",
    "href": "cherry_blossoms.html",
    "title": "Predicting Bloom Date of Cherry Blossoms",
    "section": "",
    "text": "A team and I competed in George Mason University’s annual competition to predict the date of cherry blossom bloom in DC. Using statistical computing, decision trees, and econometrics in R, our poster is below."
  },
  {
    "objectID": "mdi.html",
    "href": "mdi.html",
    "title": "Massive Data Institute",
    "section": "",
    "text": "For much of my time at Georgetown, I worked at the Massive Data Institute at the McCourt School of Public Policy, on their Environmental Impact Data Collaborative Team. The EIDC works with researchers in government, academia, and industry to solve climate problems using big data to identify the most at risk communities who may be eligible for climate funding.\nOne of my major projects was building a tool to move data between geographic scales (i.e. congressional districts to counties). When dealing with digital shapefiles that come in bizarre and gerrymandered shapes, many nonprofits and academics need to subdivide or join data according to new shape boundaries. To do this, we developed a tool in Python that can intake data at one geographic scale, calculate the population density weighted partitions of one spatial unit, and output the data at a new geographic scale.\nThe original article outlinining this can be found here: MDI’s Spatial Toolkit\nUse cases:\n\nMoving between counties and congressional districts when calculating Green Jobs in California\nMoving between water utility districts and census tracts to identify communities who could receive funding to address unsafe drinking water\n\nSpatial Misalignment between CA Districts and Counties"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "I currently work with Boeing’s Global Services applying classical and cutting edge machine learning techniques to aircraft health analysis. My work is largely with the United States’ military, using predictive analytics to keep our fleets and active service members mission ready.\nI completed my MS in Data Science from Georgetown in Spring 2023 after completing my BA in Economics and Quantitative Social Science from Dartmouth College in 2021. Academically and professionally, my goal is to use data science to do good in the world - whether that’s keeping aircrafts safe, identifying vulnerable communities for climate funding, or investigating hate speech/fraudulent text on social media.\nI love being able to explore the data behind these problems in order to generate meaningful and defensible change - and delivering complex solutions in concise and accessible ways to move that needle is where my greatest strengths lie.\nOutside of Data Science I love being outside - skiing, biathlon, climbing, biking, backpacking, and roller blading, especially with friends, is the best part of life! Plus, I love a good book. \nFeel free to reach out if you have questions about any of this work or just wanna chat. \nEmail: eliserust[at]gmail.com"
  },
  {
    "objectID": "nn.html",
    "href": "nn.html",
    "title": "Neural Networks",
    "section": "",
    "text": "As part of a class in Neural Networks, my team and I built an artificial neural network (ANN) from scratch as part of a comparative analysis with traditional ML methods in classifying derby winners.\nUsing data scraped from a weather API, Equiibase’s race history statistics PDFs, and from Kaggle’s Big Data Derby Competition, we employed NLP data pre-processing to prepare for modeling. Feature importance and hyper parameter tuning were used to gain insight into the data and tune the models.\nAn ANN model that processed numerical, categorical, and text data was built to predict the target variables using the Keras functional API. Different input layers were specified for different data types. Embedding was performed for all categorical features; the dimensions were specified via the rule of thumb of taking the minimum number of fifty and the number of categories divided by two. Then, the embeddings for all categorical features were concatenated. The text sequences for each observation were tokenized, padded, and fed to the input layer. After that, we created embeddings for the text data input and processed them with RNN layers, such as the LSTM. Numerical variables were fed to a dense layer. A concatenation layer was created to combine all outputs from above. More dense layers were built to provide more non-linearity to the model.\nCode"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elise Rust",
    "section": "",
    "text": "Currently an Engineering Data Scientist with Boeing’s Global Services sector, using machine learning, artificial neural networks, and natural language processing to analyze and forecast the health of aircrafts for our military customers.\nPrior to my time with Boeing, I received my Masters in Data Science and Analytics from Georgetown University, with a focus on sentiment analysis of political discourse and statistical modeling of climate change.\nI grew a love for data (plus skiing and hiking) at Dartmouth College!\nHere’s a resume\n \nCheck out some of my projects to the left"
  },
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "What I’m Reading",
    "section": "",
    "text": "Bookshelf"
  },
  {
    "objectID": "covid.html",
    "href": "covid.html",
    "title": "COVID-19 Vulnerability",
    "section": "",
    "text": "As part of my Probabilistic Modeling and Statistical Computing course in R for my masters degree, a group and I employed conditional probability models, hypothesis testing, and multivariate regression analysis to examine inequality in risk and exposure to COVID nationwide, with special focus on college campuses and prisons. As part of the study, we sought to ask:\n1) Do people in America face different risk profiles in contracting COVID-19 based on their location?  2) Are some subsets of the American population more susceptible? Are some subsets better equipped to avoid contracting COVID-19?  3) What factors influence this risk? \nThe primary risk factors examined were population size, mask usage, and political leaning.\nCode is at the bottom of the report, and hosted here."
  }
]